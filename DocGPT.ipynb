{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkRbPOeNm8XI",
        "outputId": "4d0f498e-0278-4a1a-9783-3fed46d7df97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m926.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.6 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.173-py3-none-any.whl (858 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m858.2/858.2 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.7 langchain-0.0.173 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.3.23-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.5.3)\n",
            "Collecting requests>=2.28 (from chromadb)\n",
            "  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.7)\n",
            "Collecting hnswlib>=0.7 (from chromadb)\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb)\n",
            "  Downloading clickhouse_connect-0.5.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.6/922.6 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers>=2.2.2 (from chromadb)\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.1)\n",
            "Collecting fastapi>=0.85.1 (from chromadb)\n",
            "  Downloading fastapi-0.95.2-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.1-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (1.26.15)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb) (2022.7.1)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb)\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb)\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.85.1->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers>=2.2.2->chromadb)\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb) (0.15.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.2->chromadb) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers>=2.2.2->chromadb)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers>=2.2.2->chromadb)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (414 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (2023.4.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb) (23.1)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (3.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers>=2.2.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers>=2.2.2->chromadb) (8.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers>=2.2.2->chromadb) (1.3.0)\n",
            "Building wheels for collected packages: hnswlib, sentence-transformers\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2119685 sha256=0e245979abaaaa20f1f3acea5036410451a6a8bf6471093b9dfc54e56a09dd0b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=92928eea458aeb3e810bd1f34e27e9d7da4db8289c19df41046ab1d3e8687536\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built hnswlib sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, monotonic, zstandard, websockets, uvloop, requests, python-dotenv, lz4, httptools, hnswlib, h11, backoff, watchfiles, uvicorn, starlette, posthog, huggingface-hub, clickhouse-connect, transformers, fastapi, sentence-transformers, chromadb\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.27.1\n",
            "    Uninstalling requests-2.27.1:\n",
            "      Successfully uninstalled requests-2.27.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.30.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 chromadb-0.3.23 clickhouse-connect-0.5.24 fastapi-0.95.2 h11-0.14.0 hnswlib-0.7.0 httptools-0.5.0 huggingface-hub-0.14.1 lz4-4.3.2 monotonic-1.6 posthog-3.0.1 python-dotenv-1.0.0 requests-2.30.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 starlette-0.27.0 tokenizers-0.13.3 transformers-4.29.2 uvicorn-0.22.0 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0.3 zstandard-0.21.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.30.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.22.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.8.1-py3-none-any.whl (248 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.8/248.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install langchain\n",
        "!pip install PyPDF2\n",
        "!pip install chromadb\n",
        "!pip install tiktoken\n",
        "!pip install pymupdf\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFm-rNk5ny-h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'sk'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5xYhQcQoJKL"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd \"/content/drive/My Drive/資料清理\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoTWnoouoYkW",
        "outputId": "a2d819df-0ce8-47dc-bf44-96729d47c772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "file_path = '/content/gdrive/My Drive/LLM/2302.12813.pdf'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mW2naVpEo5lz"
      },
      "source": [
        "# 讀取 PDF 檔案的多種方法"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow8oHTBEo6hq"
      },
      "source": [
        "* PyPdf2\n",
        "* PyPdfLoader\n",
        "* PyMuPDFLoader\n",
        "\n",
        "[官方文件](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/pdf.html#pdf)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gGE5zTL8pGrh"
      },
      "source": [
        "### 讀取 PDF 檔案 - PyPDF2\n",
        "\n",
        "傳統方法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JMVB0ywpITO"
      },
      "outputs": [],
      "source": [
        "# 從 PyPDF2 引入 PdfReader class 進行作用\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# 擺放 Pdf 的檔案路徑\n",
        "pdfreader = PdfReader(file_path)\n",
        "\n",
        "# 從路徑中讀取檔案，並且放進名為 raw_text 變數當中做存放\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "    text = page.extract_text()\n",
        "    if text:\n",
        "        raw_text += text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YCVs2FFupT3Z",
        "outputId": "b5f97495-2506-42fa-b0f6-b7a3b37eb63a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Check Your Facts and Try Again: Improving Large Language Models\\nwith External Knowledge and Automate'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_text[:100]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wm2RdrPQpeIw"
      },
      "source": [
        "## 讀取 PDF 檔案 - PyPdfLoader"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AGXmdBJfpive"
      },
      "source": [
        "用 pypdf 將 PDF 載入成包含頁面內容和頁碼元資料的文件陣列。每個文件都包含了該頁面的內容和元資料，例如頁碼等"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-vt9zp3piQR"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "pypdf_loader = PyPDFLoader(file_path)\n",
        "pypdf_doc = pypdf_loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQb7Zl5gqv88",
        "outputId": "36f6d54f-7ee1-4ef9-daf5-4e179bc91cc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Galaxy player transfer), LLM-A UGMENTER ﬁrst\\nretrieves evidence from external knowledge ( e.g.,\\nWeb or task-speciﬁc datasets) and, if necessary,\\nfurther consolidates evidence by linking retrieved\\nraw evidence with related context ( e.g., informa-\\ntion of the entity “2013 Los Angeles Galaxy”)\\nand performing reasoning to form evidence chains\\n(e.g., table-passage in the ﬁgure). Then, LLM-\\nAUGMENTER queries a ﬁxed LLM ( i.e.,ChatGPT\\nin our study) using a prompt that contains the con-\\nsolidated evidence for ChatGPT to generate a can-\\ndidate response grounded in external knowledge\\n(evidence). LLM-A UGMENTER then veriﬁes the\\ncandidate response e.g., by checking whether it\\nhallucinates evidence. If so, LLM-A UGMENTER\\ngenerates a feedback message ( e.g.,about the team\\n“C.S.D. Municipal”). The message is used to re-\\nvise the prompt to query ChatGPT again. The pro-\\ncess iterates until a candidate response passes the\\nveriﬁcation and is sent to the user.\\nIn addition to proposing LLM-A UGMENTER ,\\nto be detailed in Section 2, we make the following\\ncontributions. We perform an empirical study to\\nvalidate the effectiveness of LLM-A UGMENTER\\nusing two tasks, information seeking dialog (Sec-\\ntion 3) and open-domain Wiki question answer-\\ning (Wiki QA) (Section 4). The study shows that\\nLLM-A UGMENTER signiﬁcantly reduces Chat-\\nGPT’s hallucinations without sacriﬁcing the ﬂu-\\nency and informativeness of its generated re-\\nsponses. For example, on the dialog task of cus-\\ntomer service, human evaluation shows LLM-\\nAUGMENTER improve ChatGPT by 32.3% in\\nUsefulness (measuring the groundedness or hal-\\nlucination of model responses) and 12.9% in Hu-\\nmanness (measuring the ﬂuency and informative-\\nness of model responses). The Wiki QA task is ex-\\ntremely challenging to ChatGPT in that answering\\nthese questions often requires multi-hop reasoning\\nto piece together information of various modali-\\nties scattered across different documents. Our re-\\nsults show that although the closed-book ChatGPT\\nperforms poorly and often hallucinates, LLM-\\nAUGMENTER substantially improves the factual-\\nity score of the answers (absolute +10% in F1) by\\ngrounding ChatGPT’s responses in consolidated\\nexternal knowledge and automated feedback.\\n2LLM-A UGMENTER\\nThe architecture of LLM-A UGMENTER is illus-\\ntrated in Figure 2. It consists of a set of PnP\\nWorking\\nMemory\\nAI AgentAction Executor\\n-Knowledge Consolidator\\n-Prompt Engine\\nLLM\\n(e.g., ChatGPT )Utility\\n(utility score & feedback)\\nEnvironmentLLM -Augmenter\\ndata flow update flowPolicy\\n(action selection)\\nExternal \\nKnowledge\\n(e.g., news, wiki, \\nproprietary \\ndatabases)Figure 2: LLM-A UGMENTER architecture showing\\nhow its plug-and-play modules interact with the LLM\\nand the user’s environment.\\nmodules ( i.e., Working Memory, Policy, Action\\nExecutor, and Utility) to improve a ﬁxed LLM\\n(e.g., ChatGPT) with external knowledge and au-\\ntomated feedback to mitigate generation problems\\nsuch as hallucination.\\nWe formulate human-system conversation as a\\nMarkov Decision Process (MDP) described by a\\nﬁve-tuple (S;A;P;R;\\r ):\\n•Sis an inﬁnite set of dialog states, which\\nencode information stored in Working Mem-\\nory, including dialog history, user query, evi-\\ndence, candidate response;\\n•Ais a set of actions that Policy picks to ex-\\necute, including (1) calling Knowledge Con-\\nsolidator to consolidate evidence from exter-\\nnal knowledge and (2) calling Prompt Engine\\nto query the LLM to generate candidate re-\\nsponses;\\n•P(s0js;a)gives the transition probability of\\nentering a new state s0after actionais taken\\nin states;\\n•R(s;a)is the external reward received af-\\nter taking action ain states, which is pro-\\nvided by the environment ( e.g., users or sim-\\nulators); and\\n•\\r2(0;1]is a discount factor.\\nIn what follows, we describe in detail the modules\\nof LLM-A UGMENTER .\\n2.1 Working Memory\\nThis module tracks the dialog state that captures\\nall essential information in the conversation so', metadata={'source': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'page': 1})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pypdf_doc[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "8ON_kzx5pvAE",
        "outputId": "4bc8f437-56ae-4c00-81d4-e2f0bc860063"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Galaxy player transfer), LLM-A UGMENTER ﬁrst\\nretrieves evidence from external knowledge ( e.g.,\\nWeb or task-speciﬁc datasets) and, if necessary,\\nfurther consolidates evidence by linking retrieved\\nraw evidence with related context ( e.g., informa-\\ntion of the entity “2013 Los Angeles Galaxy”)\\nand performing reasoning to form evidence chains\\n(e.g., table-passage in the ﬁgure). Then, LLM-\\nAUGMENTER queries a ﬁxed LLM ( i.e.,ChatGPT\\nin our study) using a prompt that contains the con-\\nsolidated evidence for ChatGPT to generate a can-\\ndidate response grounded in external knowledge\\n(evidence). LLM-A UGMENTER then veriﬁes the\\ncandidate response e.g., by checking whether it\\nhallucinates evidence. If so, LLM-A UGMENTER\\ngenerates a feedback message ( e.g.,about the team\\n“C.S.D. Municipal”). The message is used to re-\\nvise the prompt to query ChatGPT again. The pro-\\ncess iterates until a candidate response passes the\\nveriﬁcation and is sent to the user.\\nIn addition to proposing LLM-A UGMENTER ,\\nto be detailed in Section 2, we make the following\\ncontributions. We perform an empirical study to\\nvalidate the effectiveness of LLM-A UGMENTER\\nusing two tasks, information seeking dialog (Sec-\\ntion 3) and open-domain Wiki question answer-\\ning (Wiki QA) (Section 4). The study shows that\\nLLM-A UGMENTER signiﬁcantly reduces Chat-\\nGPT’s hallucinations without sacriﬁcing the ﬂu-\\nency and informativeness of its generated re-\\nsponses. For example, on the dialog task of cus-\\ntomer service, human evaluation shows LLM-\\nAUGMENTER improve ChatGPT by 32.3% in\\nUsefulness (measuring the groundedness or hal-\\nlucination of model responses) and 12.9% in Hu-\\nmanness (measuring the ﬂuency and informative-\\nness of model responses). The Wiki QA task is ex-\\ntremely challenging to ChatGPT in that answering\\nthese questions often requires multi-hop reasoning\\nto piece together information of various modali-\\nties scattered across different documents. Our re-\\nsults show that although the closed-book ChatGPT\\nperforms poorly and often hallucinates, LLM-\\nAUGMENTER substantially improves the factual-\\nity score of the answers (absolute +10% in F1) by\\ngrounding ChatGPT’s responses in consolidated\\nexternal knowledge and automated feedback.\\n2LLM-A UGMENTER\\nThe architecture of LLM-A UGMENTER is illus-\\ntrated in Figure 2. It consists of a set of PnP\\nWorking\\nMemory\\nAI AgentAction Executor\\n-Knowledge Consolidator\\n-Prompt Engine\\nLLM\\n(e.g., ChatGPT )Utility\\n(utility score & feedback)\\nEnvironmentLLM -Augmenter\\ndata flow update flowPolicy\\n(action selection)\\nExternal \\nKnowledge\\n(e.g., news, wiki, \\nproprietary \\ndatabases)Figure 2: LLM-A UGMENTER architecture showing\\nhow its plug-and-play modules interact with the LLM\\nand the user’s environment.\\nmodules ( i.e., Working Memory, Policy, Action\\nExecutor, and Utility) to improve a ﬁxed LLM\\n(e.g., ChatGPT) with external knowledge and au-\\ntomated feedback to mitigate generation problems\\nsuch as hallucination.\\nWe formulate human-system conversation as a\\nMarkov Decision Process (MDP) described by a\\nﬁve-tuple (S;A;P;R;\\r ):\\n•Sis an inﬁnite set of dialog states, which\\nencode information stored in Working Mem-\\nory, including dialog history, user query, evi-\\ndence, candidate response;\\n•Ais a set of actions that Policy picks to ex-\\necute, including (1) calling Knowledge Con-\\nsolidator to consolidate evidence from exter-\\nnal knowledge and (2) calling Prompt Engine\\nto query the LLM to generate candidate re-\\nsponses;\\n•P(s0js;a)gives the transition probability of\\nentering a new state s0after actionais taken\\nin states;\\n•R(s;a)is the external reward received af-\\nter taking action ain states, which is pro-\\nvided by the environment ( e.g., users or sim-\\nulators); and\\n•\\r2(0;1]is a discount factor.\\nIn what follows, we describe in detail the modules\\nof LLM-A UGMENTER .\\n2.1 Working Memory\\nThis module tracks the dialog state that captures\\nall essential information in the conversation so'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pypdf_doc[1].page_content"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0yv0IMhuqy_y"
      },
      "source": [
        "## 讀取 PDF 檔案 - PyMuPDFLoader"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UUp2dEhuq1s_"
      },
      "source": [
        "這是 PDF 解析選項中最快的，並且包含有關 PDF 及其頁面的詳細元資料，以期高渲染速度聞名。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGrfMLEKq3my"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "pyMuPDFLoader = PyMuPDFLoader(file_path)\n",
        "pyMuPDF_doc = pyMuPDFLoader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jQfVHDBq7XB",
        "outputId": "170e8e37-93cf-48a5-c550-202810eba741"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Check Your Facts and Try Again: Improving Large Language Models\\nwith External Knowledge and Automated Feedback∗\\nBaolin Peng† Michel Galley† Pengcheng He† Hao Cheng† Yujia Xie†\\nYu Hu† Qiuyuan Huang† Lars Liden† Zhou Yu‡ Weizhu Chen† Jianfeng Gao†\\n† Microsoft Research\\n‡ Columbia University\\nAbstract\\nLarge language models (LLMs), such as Chat-\\nGPT, are able to generate human-like, ﬂu-\\nent responses for many downstream tasks,\\ne.g., task-oriented dialog and question an-\\nswering.\\nHowever, applying LLMs to real-\\nworld, mission-critical applications remains\\nchallenging mainly due to their tendency to\\ngenerate hallucinations and their inability to\\nuse external knowledge. This paper proposes a\\nLLM-AUGMENTER system, which augments\\na black-box LLM with a set of plug-and-play\\nmodules. Our system makes the LLM gen-\\nerate responses grounded in external knowl-\\nedge, e.g., stored in task-speciﬁc databases. It\\nalso iteratively revises LLM prompts to im-\\nprove model responses using feedback gen-\\nerated by utility functions, e.g., the factuality\\nscore of a LLM-generated response. The ef-\\nfectiveness of LLM-AUGMENTER is empiri-\\ncally validated on two types of scenarios, task-\\noriented dialog and open-domain question an-\\nswering. LLM-AUGMENTER signiﬁcantly re-\\nduces ChatGPT’s hallucinations without sac-\\nriﬁcing the ﬂuency and informativeness of its\\nresponses. We make the source code and mod-\\nels publicly available.1\\n1\\nIntroduction\\nLarge Language models (LLMs), such as GPT-3\\n(Brown et al., 2020) and ChatGPT, have demon-\\nstrated an outstanding ability in generating ﬂu-\\nent, coherent, and informative natural language\\ntexts. It is commonly understood that the impres-\\nsive capabilities of these models stem from the\\nabundance of world knowledge encoded therein\\nand models’ ability to generalize from that knowl-\\nedge. However, the knowledge encoding of LLMs\\nis lossy and the knowledge generalization could\\nlead to “memory distortion.” As a result, these\\nmodels tend to hallucinate, which can cause dam-\\nage when deployed for mission-critical tasks. In\\n∗Correspondence: {bapeng,mgalley,jfgao}microsoft.com\\n1https://aka.ms/llm-augmenter\\nAI Agent (LLM-Augmenter + LLM)\\nRevise response via automatic feedback\\nConsolidate evidence from external knowledge\\nWhich 2013 Los Angeles Galaxy player transferred in from the team with 12 \\ninternational titles ?\\nCandidate response:\\nJaime Penedo is transferred in from C.S.D. \\nMunicipal, a team with 12 international titles.\\nFeedback:\\nThe player Jaime Penedo is transferred in from \\nC.S.D. Municipal, but there is no information \\nabout the number of international titles of this \\nteam.\\nRevised candidate response:\\nJuninho is transferred in from São Paulo, a team \\nwith 12 international titles.\\nJuninho is transferred in from São Paulo, a team with 12 international titles.\\nFigure 1: LLM-AUGMENTER improves a ﬁxed LLM\\nby (1) consolidating evidence from external knowledge\\nfor the LLM to generate responses grounded in evi-\\ndence, and (2) revising LLM’s (candidate) responses\\nusing automated feedback.\\naddition, even with exponentially growing model\\nsizes, LLMs can never encode all information\\nneeded for many applications. For example, con-\\nstant changes in real-world settings cause LLMs\\nto quickly become stale for time-sensitive tasks\\nsuch as news question answering, and many pro-\\nprietary datasets are not available for LLM train-\\ning due to privacy. While there is a growing inter-\\nest in improving LLMs using external knowledge\\n(e.g., Ghazvininejad et al., 2017; Guu et al., 2020;\\nZhong et al., 2022; Gao et al., 2019, 2022), al-\\nmost all the previously proposed methods require\\nﬁnetuning the parameters of a LLM, which can be\\nprohibitively expensive as the size of LLMs grows\\nexponentially. Thus, it is highly desirable to aug-\\nment a ﬁxed LLM with plug-and-play (PnP) mod-\\nules for mission-critical tasks.\\nIn this paper, we present LLM-AUGMENTER\\nto improve LLMs with external knowledge and\\nautomated feedback using PnP modules. As il-\\nlustrated by the example in Figure 1, given a\\nuser query (e.g., regarding a 2013 Los Angeles\\narXiv:2302.12813v3  [cs.CL]  8 Mar 2023\\n', metadata={'source': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'file_path': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230310011653Z', 'modDate': 'D:20230310011653Z', 'trapped': ''})"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pyMuPDF_doc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XDtUUAecq-mX",
        "outputId": "c10f99fb-6ad4-4954-987d-873c10815337"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LaTeX with hyperref'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pyMuPDF_doc[2].metadata['creator']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fQczdSrvraYa"
      },
      "source": [
        "# 使用 QA_Chain 與文件對話"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DxZg1_LCrb8X"
      },
      "source": [
        "Langchain中的 load_qa_chain 函數用於加載一個可以用來回答問題的 Chain。\n",
        "\n",
        "chain_type 參數可用於指定加載的鏈的類型。 chain_type 的可能值為 stuff, map_reduce, refine, map_rerank。\n",
        "\n",
        "* Stuff: 把所有的文本一次性傳給 LLM 進行總結。如果文本長度超過 LLM Token 時將會炸裂，對長文不會使用這個方式。（一般來說都不會用就是）\n",
        "* map_reduce: 此法是先將文本分成多個小 batch 後，並針對每個小 batch 進行總結。\n",
        "* refine: 此法將文本分成多個小 Batch 之後，有順序的先對第一個 batch 總結，之後結合第二個 batch 進行總結，以此類推，可以增加上下文連貫性。\n",
        "* map_rerank: 此法比較像是 Retrival ，會將文本與提出的問題進行比對，找到最接近的那一項，接著交給 LLM，在接手 LLM 的回答。\n",
        "\n",
        "[官方文件](https://python.langchain.com/en/latest/modules/chains/index_examples/question_answering.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_YJjH7PrwV3"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "# 讀取文件\n",
        "pyMuPDFLoader = PyMuPDFLoader(file_path)\n",
        "pyMuPDF_doc = pyMuPDFLoader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3abI_Kq8tGoS",
        "outputId": "160090dd-ecc5-4721-9df6-5092a05e1c12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "langchain.schema.Document"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a=pyMuPDF_doc[0]\n",
        "type(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "6GdtsFg7rzjL",
        "outputId": "c0ea53c4-761f-4a1e-d034-09cbba5f490d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The main point of this paper is to present LLM-AUGMENTER, a system which augments a black-box LLM with plug-and-play modules to make it generate responses grounded in external knowledge and improve model responses using automated feedback.'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 建構 QA_chain, Chain_type 有 stuff, map_reduce, refine, map_rerank\n",
        "chain = load_qa_chain(llm=OpenAI(), chain_type=\"stuff\") #當你出現 error 4097 token 錯誤資訊時，將 stuff 改成 map_reduce\n",
        "# 問問題時間！\n",
        "query = \"what is the main point?\"\n",
        "chain.run(input_documents=[pyMuPDF_doc[0]], question=query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "addU0KU7uSEg"
      },
      "source": [
        "## 切割文件機制"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GV3opoasuWcq"
      },
      "source": [
        "Langchain 中的 Text Splitters 是一種工具，可用於將長文本拆分為更小、更易於管理的塊。這可用於各種任務，例如：\n",
        "\n",
        "* 總結文本：通過將文本分成較小的塊，可以更容易地識別文本的要點並以簡潔的方式進行總結。\n",
        "* 索引文本：通過將文本拆分成更小的塊，可以更輕鬆地索引文本，以便更有效地搜索它。\n",
        "\n",
        "Langchain 中的 Text Splitters 是一個強大的工具，可以通過多種方式對文本進行處理和分析。如果您正在處理大段文本，那麼 Text Splitters 可能是你正在尋找的"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBXNUzmeuTUD"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTYi36PEufAN"
      },
      "outputs": [],
      "source": [
        "pages = pyMuPDFLoader.load_and_split(splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0_GJEZsuhLC",
        "outputId": "fae56c1f-9894-4183-8a3f-7e745e880283"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Check Your Facts and Try Again: Improving Large Language Models\\nwith External Knowledge and Automated Feedback∗\\nBaolin Peng† Michel Galley† Pengcheng He† Hao Cheng† Yujia Xie†\\nYu Hu† Qiuyuan Huang† Lars Liden† Zhou Yu‡ Weizhu Chen† Jianfeng Gao†\\n† Microsoft Research\\n‡ Columbia University\\nAbstract\\nLarge language models (LLMs), such as Chat-\\nGPT, are able to generate human-like, ﬂu-\\nent responses for many downstream tasks,\\ne.g., task-oriented dialog and question an-\\nswering.\\nHowever, applying LLMs to real-\\nworld, mission-critical applications remains\\nchallenging mainly due to their tendency to\\ngenerate hallucinations and their inability to\\nuse external knowledge. This paper proposes a\\nLLM-AUGMENTER system, which augments\\na black-box LLM with a set of plug-and-play\\nmodules. Our system makes the LLM gen-\\nerate responses grounded in external knowl-\\nedge, e.g., stored in task-speciﬁc databases. It\\nalso iteratively revises LLM prompts to im-\\nprove model responses using feedback gen-', metadata={'source': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'file_path': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230310011653Z', 'modDate': 'D:20230310011653Z', 'trapped': ''})"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pages[0]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dKnqAH8MvdNz"
      },
      "source": [
        "# 向量化小文本並放進向量資料庫中"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4FUPDYgwveKP"
      },
      "source": [
        "## 讀取 Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjL0eJ__vgbr"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pq1u_fIvi31"
      },
      "outputs": [],
      "source": [
        "# Download embeddings from OpenAI\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "esoFMmonwYwR"
      },
      "source": [
        "### 存放至 Chroma - 向量資料庫"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqtHjKoGwZpH"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JirTKktRwdAz",
        "outputId": "4222fe7f-1755-4386-c037-8f3f23e13d66"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: db\n"
          ]
        }
      ],
      "source": [
        "db = Chroma.from_documents(documents=pages, embedding=embeddings, persist_directory='db')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LWDsEb-wf_n",
        "outputId": "b3777af5-4192-43c4-d440-506a69711554"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain.vectorstores.chroma.Chroma at 0x7f13170d4c10>"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "db"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jPGD9KwyxuNB"
      },
      "source": [
        "# 多種使用 Chain 與文件對話的方式"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pWB1B9vGxu36"
      },
      "source": [
        "* RetrievalQA\n",
        "* ConversationalRetrievalChain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "I3Vt0w0pxzFs"
      },
      "source": [
        "## RetrievalQA"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JdU1wtO_x0BE"
      },
      "source": [
        "RetrievalQA Chain 用意是在通過檢索向量資料庫中與問題最相近的向量進行回答，當中我們引入新的觀念 Retriever，透過 Retriever 來進行比較相似度，方法有兩個，分別為'mmr' 與 'similarity'，還可以設置要比較的數量\n",
        "\n",
        "[Chain 官方連結](https://python.langchain.com/en/latest/modules/chains/index_examples/vector_db_qa.html)\n",
        "\n",
        "[VectorStore Retriever](https://python.langchain.com/en/latest/modules/indexes/retrievers/examples/vectorstore-retriever.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0V3tH227yLpH",
        "outputId": "e14d16bc-0795-478e-8454-ca4f71447229"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# 讀取文件\n",
        "pyMuPDFLoader = PyMuPDFLoader(file_path)\n",
        "pyMuPDF_doc = pyMuPDFLoader.load()\n",
        "\n",
        "# 切割文件\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "#pages = pyMuPDFLoader.load_and_split(splitter)\n",
        "pages = splitter.split_documents(pyMuPDF_doc)\n",
        "\n",
        "# 選擇要用的 embedding 並存放進向量資料庫\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = Chroma.from_documents(pages, embeddings)\n",
        "\n",
        "# 使用 retrievaler 進行檢索\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})\n",
        "\n",
        "# 建構 QA Chain 來進行問答\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw6ZR9FtyZi1"
      },
      "outputs": [],
      "source": [
        "query = \"what is the main point in this document?\"\n",
        "result = qa({\"query\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Wn5EOsJ6ydGc",
        "outputId": "513609d9-a000-40e0-bb37-f5bdf54e9b1e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The main point of this document is to demonstrate the effectiveness of LLM-AUGMENTER using two tasks, information seeking dialog and open-domain Wiki question answering.'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjt5BV8_yebo",
        "outputId": "04923d7f-570d-4a64-b906-c15049905564"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='to improve LLMs with external knowledge and\\nautomated feedback using PnP modules. As il-\\nlustrated by the example in Figure 1, given a\\nuser query (e.g., regarding a 2013 Los Angeles\\narXiv:2302.12813v3  [cs.CL]  8 Mar 2023', metadata={'source': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'file_path': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'page': 0, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230310011653Z', 'modDate': 'D:20230310011653Z', 'trapped': ''}),\n",
              " Document(page_content='in the description, which were crawled from var-\\nious news-related subreddits during the time pe-\\nriod of 2021-2022. We then restricted the URL\\ndomain to a curated list of news websites, and ex-\\ntracted the relevant oracle passage by selecting the\\nmost appropriate passage for the context based on\\nROUGE-F1 scores (Lin, 2004).\\nIn order to re-\\nduce noisy or irrelevant information, we only kept\\nexamples with an F1 score higher than a certain\\nthreshold, resulting in a total of 1370 examples for\\nevaluation.', metadata={'source': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'file_path': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'page': 3, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230310011653Z', 'modDate': 'D:20230310011653Z', 'trapped': ''}),\n",
              " Document(page_content='essary support evidence in one-shot. Thus, more\\nadvanced knowledge consolidation techniques are\\nessential to elicit LLMs for proper grounding.\\nLastly, different from conversational tasks where\\nlong-form responses are desirable, we mainly con-\\nsider questions with concise short-form answers,\\ni.e., there exists a signiﬁcant style shift in re-\\nsponses. To align ChatGPT to this new scenario\\nwith distinct characteristics, extra instructions are\\nneeded.\\n4.1\\nDataset\\nOTT-QA:\\nThe OTT-QA dataset is an open-\\ndomain question answering benchmark that con-\\nsiders multi-step joint reasoning over both tabular\\nand textual information. It consists of around 40K\\ninstances built upon Wikipedia, including 400K\\ntables and 6M passages as the knowledge source.\\nSolving the questions in OTT-QA requires diverse\\nreasoning skills and can be divided into three\\ncategories: single-hop questions (13%), two-hop\\nquestions (57%), and multi-hop questions (30%).\\nIn this paper, we denote the dataset as Wiki QA.\\n4.2', metadata={'source': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'file_path': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'page': 7, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230310011653Z', 'modDate': 'D:20230310011653Z', 'trapped': ''}),\n",
              " Document(page_content='to be detailed in Section 2, we make the following\\ncontributions. We perform an empirical study to\\nvalidate the effectiveness of LLM-AUGMENTER\\nusing two tasks, information seeking dialog (Sec-\\ntion 3) and open-domain Wiki question answer-\\ning (Wiki QA) (Section 4). The study shows that\\nLLM-AUGMENTER signiﬁcantly reduces Chat-\\nGPT’s hallucinations without sacriﬁcing the ﬂu-\\nency and informativeness of its generated re-\\nsponses. For example, on the dialog task of cus-\\ntomer service, human evaluation shows LLM-\\nAUGMENTER improve ChatGPT by 32.3% in\\nUsefulness (measuring the groundedness or hal-\\nlucination of model responses) and 12.9% in Hu-\\nmanness (measuring the ﬂuency and informative-\\nness of model responses). The Wiki QA task is ex-\\ntremely challenging to ChatGPT in that answering\\nthese questions often requires multi-hop reasoning\\nto piece together information of various modali-\\nties scattered across different documents. Our re-\\nsults show that although the closed-book ChatGPT', metadata={'source': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'file_path': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'page': 1, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230310011653Z', 'modDate': 'D:20230310011653Z', 'trapped': ''}),\n",
              " Document(page_content='ing Memory is insufﬁcient for prompting LLMs.\\nThus, we further use additional intermediary mod-\\nules, i.e., linker and chainer, from CORE (Ma\\net al., 2022) to consolidate the raw evidence, in-\\ncluding connecting relevant documents, reranking\\nevidence, and splicing them into evidence chains.\\nWe refer to Ma et al. (2022) for more details.\\nPrompt Engine:\\nThe prompt templates utilized\\nfor Wiki QA is shown in the appendix in Table 9.', metadata={'source': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'file_path': '/content/gdrive/My Drive/LLM/2302.12813.pdf', 'page': 7, 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20230310011653Z', 'modDate': 'D:20230310011653Z', 'trapped': ''})]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"source_documents\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dwNQJGeEzCqB"
      },
      "source": [
        "## ConversationalRetrievalChain"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "77eYOlswzHWI"
      },
      "source": [
        "[官方連結](https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html#chat-over-documents-with-chat-history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7XIC4vUzDYc",
        "outputId": "0f33f7f6-f143-49a7-83b5-0dccdac99fb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "# 讀取文件\n",
        "pyMuPDFLoader = PyMuPDFLoader(file_path)\n",
        "pyMuPDF_doc = pyMuPDFLoader.load()\n",
        "\n",
        "# 切割文件\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "#pages = pyMuPDFLoader.load_and_split(splitter)\n",
        "pages = splitter.split_documents(pyMuPDF_doc)\n",
        "\n",
        "# 選擇要用的 embedding 並存放進向量資料庫\n",
        "embeddings = OpenAIEmbeddings()\n",
        "db = Chroma.from_documents(pages, embeddings)\n",
        "\n",
        "# 使用 retrievaler 進行檢索\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5})\n",
        "\n",
        "# 建構 Memorry 來保存聊天記錄\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# 建構 QA Chain 來進行問答\n",
        "qa = ConversationalRetrievalChain.from_llm(\n",
        "    llm=OpenAI(), chain_type=\"stuff\", retriever=retriever, memory=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Az2AbbGGzQ0G"
      },
      "outputs": [],
      "source": [
        "query = \"what is the main point in this document?\"\n",
        "result = qa({\"question\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "gA-27KiLzT66",
        "outputId": "e01af0ba-f36e-4528-c15b-1f0e3496723a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' This document describes a method to improve Language Models with external knowledge and automated feedback using Plug-and-Play modules. It also introduces a new open-domain question answering benchmark called OTT-QA, which consists of around 40,000 instances built upon Wikipedia, including 400,000 tables and 6 million passages as the knowledge source. Lastly, it discusses additional methods to align ChatGPT to a new scenario with distinct characteristics.'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECstpT-gzdFI"
      },
      "outputs": [],
      "source": [
        "query = \"can you rewrite it?\"\n",
        "result = qa({\"question\": query})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "ucSUq4O8zeUF",
        "outputId": "7dc4b063-5f29-418d-a74e-2d018aeb2f09"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' This document explains how to use the OTT-QA dataset and the Wiki QA dataset to evaluate different models for customer service scenarios. It also describes the Knowledge Consolidator used and the experiment setups used.'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result[\"answer\"]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
