{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fvwHFbXx3Ili"
      },
      "source": [
        "[LangChain Docummentation](https://python.langchain.com/en/latest/index.html)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iJQFQ5le3VY_"
      },
      "source": [
        "需先安裝openAI、LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDwHtJJd3RjY",
        "outputId": "670dcf65-06f1-49c6-d32b-f77064e72451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.173-py3-none-any.whl (858 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m858.2/858.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.173 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.8.0 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.6\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install openai"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Yycwc93shC"
      },
      "source": [
        "### 載入OpenAI key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1D8Fs_S2uTy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'sk-'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VSapSqO34Mbg"
      },
      "source": [
        "[modules](https://python.langchain.com/en/latest/modules/models.html): 為 Langchain 當中支持的各種大型語言模型，如：OpenAI、HuggingFace、cohere 等。\n",
        "\n",
        "\n",
        "[prompt](https://python.langchain.com/en/latest/modules/prompts.html): 進行 Prompt 管理，序列化。\n",
        "\n",
        "[chain](https://python.langchain.com/en/latest/modules/chains.html): LangChain 為 Chain 提供標準接口、與其他工具的大量集成以及用於常見應用程序的端到端鏈。\n",
        "\n",
        "[Agent](https://python.langchain.com/en/latest/modules/agents.html): LangChain 為 Agent 提供了一個標準接口，可供選擇的代理選擇，以及端到端代理的示例。例如：\"zero-shot-description\" 等\n",
        "\n",
        "[Memory](https://python.langchain.com/en/latest/modules/memory.html): LangChain 提供了 Memory 標準接口、 Memory 實現的集合以及使用內存的鏈/代理的示例。"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rFPE7Qz692HH"
      },
      "source": [
        "## Models: 透過語言模型，取得摘要、推理、生成能力"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ20d4NP9-Db"
      },
      "source": [
        "LangChain 中最基本的元素 Model，底下可以透過簡單的例子示範如何操作。 為此，讓我們假設一下正在打造一個服務，專門提供公司名稱，只要給我們公司描述，就給出建議的公司命名。\n",
        "\n",
        "為了可以實現這個目標，我們首先 import OpenAI 來執行，model-name 可以選擇 'text-davinci-003'，結束後，基本上你會覺得跟操作 ChatGPT 類似，如此一來你就完成了一個 LLM 應用程式。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zI330W9966A"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eR7rt49-Lh3"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model_name=\"text-davinci-003\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrV2LHzj-NuB",
        "outputId": "cf5e2a46-5fdd-4fca-973e-c7e47e15ce85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "RainbowSolez\n"
          ]
        }
      ],
      "source": [
        "text = \"Give a good company name for that makes colorful shoes?\"\n",
        "print(llm(text))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Aeym0A7bASYa"
      },
      "source": [
        "## Prompt: 替大型語言模型(LLM) 管理提示語(prompt)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gc-2zfrbAT-3"
      },
      "source": [
        "常在應用程式中呼叫 LLM 時，你不會直接將使用者撰寫的文字傳送至 LLM。相反的，你可能將使用者撰寫的文字進行組合，再傳送至 LLM 當中。\n",
        "\n",
        "例如，在前面的例子中，我們是要給定公司產品後，產生出合適的公司名稱。既然如此，我們可以把公司產品名稱作為參數，以此打造一個 Prompt。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elWT_JuSATjc"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"], #input 變數\n",
        "    template=\"What is a good name for a company that makes {product}?\",  #prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3pJmUpACRXK",
        "outputId": "c3648bd5-7d24-47bf-ea4a-ee15f07aad53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is a good name for a company that makes colorful shoes?\n"
          ]
        }
      ],
      "source": [
        "print(prompt.format(product=\"colorful shoes\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bT5qLwE8CXTi",
        "outputId": "d2ac73d3-3de9-4fa2-e342-34ae0947608f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Rainbow Footwear.\n"
          ]
        }
      ],
      "source": [
        "print(llm(prompt.format(product=\"colorful shoes\")))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rlv9japcDDmD"
      },
      "source": [
        "## Chains: 結合 LLM 與 prompts 於多個步驟工作流中"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bGNVSZQMDolg"
      },
      "source": [
        "在 LangChain 中，Chain 可以是由 llm 或是其他 Chain 連結接在一起。\n",
        "\n",
        "最核心的 Chain 是 LLMChain，它由 PromptTemplate 和 LLM 結合。\n",
        "\n",
        "延續前面了範例，這個環節中，我們建構一個 LLMChain，這個 Chain 會接受使用者輸入，並且使用 PromptTemplate 將其規格化，最後將規格化內容傳遞給 LLM。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwJs_CLIDt6v"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6HIE1phEEv2"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4NVxli95EINy",
        "outputId": "30bbb6ea-b825-476e-ec7b-759dcec60a45"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nRainbow Footwear.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run(\"colorful shoes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjLdKzvQHcCj"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List\n",
        "from langchain.chains.base import Chain\n",
        "class ConcatenateChain(Chain):\n",
        "    chain_1: LLMChain\n",
        "    chain_2: LLMChain\n",
        "    @property\n",
        "    def input_keys(self) -> List[str]:\n",
        "        all_input_vars = set(self.chain_1.input_keys).union(set(self.chain_2.input_keys))\n",
        "        return list(all_input_vars)\n",
        "\n",
        "    @property\n",
        "    def output_keys(self) -> List[str]:\n",
        "        return ['concat_output']\n",
        "\n",
        "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
        "        output_1 = self.chain_1.run(inputs)\n",
        "        output_2 = self.chain_2.run(inputs)\n",
        "        return {'concat_output': output_1 + output_2}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_F17yLFHOWt",
        "outputId": "9919eefb-91c9-493c-944e-8bbca0e5d3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Concatenated output:\n",
            "\n",
            "\n",
            "Kaleidoscope Kicks.\n",
            "\n",
            "\"Brighten up your day with Colorful Socks!\"\n"
          ]
        }
      ],
      "source": [
        "prompt_1 = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")\n",
        "chain_1 = LLMChain(llm=llm, prompt=prompt_1)\n",
        "\n",
        "prompt_2 = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good slogan for a company that makes {product}?\",\n",
        ")\n",
        "chain_2 = LLMChain(llm=llm, prompt=prompt_2)\n",
        "\n",
        "concat_chain = ConcatenateChain(chain_1=chain_1, chain_2=chain_2)\n",
        "concat_output = concat_chain.run(\"colorful socks\")\n",
        "print(f\"Concatenated output:\\n{concat_output}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-o_UKX8JH0Ws"
      },
      "source": [
        "## Agents: 讓 LLM 選擇要用的 tool 解決問題"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zKBKAqM5H93p"
      },
      "source": [
        "到目前為止，看到的應用都是是按照預定的順序運行的，特別是像 Chain 用法。\n",
        "\n",
        "然而，Agnet 不再如此，能夠使用 LLM 來確定要採取的動作及順序。動作可以是使用工具並觀察其輸出，也可以是返回結果給用戶。如果正確使用， Agent 可以非常強大。\n",
        "\n",
        "接觸 Agnets ，應該會了解以下概念：\n",
        "\n",
        "* tool：執行特定職責的功能。這可以是 Google搜索、查找資料庫、Python REPL、其他Chain 等等。\n",
        "* LLM：大型語言模型提供 Agents 決定要執行的動作以及順序。\n",
        "* Agent：要使用的代理。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwG_w7gJIRI-",
        "outputId": "8c8490e6-68df-4882-a6c8-6d1a39ee3458"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32002 sha256=8b7595ec69b99c93c5ab7113a0c5a6dbfba1310aea0d6b8de1f4055177cdb29f\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3n9h7MNIUV1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"169f6c497ae97c71bfe15b079d28cf315ca9eabbfad75c60b520c25474a323a9\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "POfuJXxJIbOR",
        "outputId": "e26da9fa-6e66-43fb-bd51-cfcc60c4ab5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out the temperatures in both Kaohsiung and Taipei.\n",
            "Action: Search\n",
            "Action Input: \"High temperature in Taiwan yesterday Kaohsiung Taipei\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mKaohsiung Weather History for the Previous 24 Hours ; 82 °F · 81 °F · 81 °F ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to compare the two temperatures\n",
            "Action: Calculator\n",
            "Action Input: 82 - 81\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The high temperature in Kaohsiung yesterday was 1 degree higher than the high temperature in Taipei yesterday.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The high temperature in Kaohsiung yesterday was 1 degree higher than the high temperature in Taipei yesterday.'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "# 第一步，讀取 llm 用來提供 Agent 做事情\n",
        "llm = OpenAI(temperature=0)\n",
        "# 第二步，讓我們添加些 tool 來提供動作，這邊選擇網路搜尋功能的 serpapi 以及計算數字用的 llm-math\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "# Now let's test it out!\n",
        "agent.run(\"What was the high temperature in Taiwan yesterday in Kaohsiung? What is that number different with the high temperature in Taiwan yesterday in Taipei?\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "34lu99JBlfw_"
      },
      "source": [
        "## Memory: 賦予 Chains 與 Agents 記憶"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KhVXSSHOllCc"
      },
      "source": [
        "Chain 以及 Agent 都是 stateless 的。但通常，我們可能會希望添加 \"記憶\" 的概念在其中，以便他可以記住先前的互動資訊。最簡單的例子就是在設計聊天機器人時，你希望他會記住過往的對話，善用上下文來進行更流暢的對談。簡單來說，這將是一種短期記憶。在複雜情況下，你可能會需要 Chain 或是 Agent 隨著時間的推移記住重要的資訊，這將是一種長期記憶。\n",
        "\n",
        "在 LangChain 當中，為了賦予記憶的概念，專門建造了幾種 Chain。以下將示範 ConversationChain，預設情況下 ConversationChain 具有一種簡單的記憶類型，能夠記住「所有」先前聊天過程，並添加在傳遞的上下文中。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knE-1CnelkjN",
        "outputId": "4d23ef80-db51-473f-f8a7-c58f44ef3025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Hi there! It's nice to meet you. How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "output = conversation.predict(input=\"Hi there!\")\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_DzU_Lrl3LF",
        "outputId": "9b973b50-b31f-4cb5-98f3-2c1803a9a759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: I am doing well! Currently, just have a conversation with you, an AI.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " That's great! It's nice to have a conversation with you too. What would you like to talk about?\n"
          ]
        }
      ],
      "source": [
        "output = conversation.predict(input=\"I am doing well! Currently, just have a conversation with you, an AI.\")\n",
        "print(output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
